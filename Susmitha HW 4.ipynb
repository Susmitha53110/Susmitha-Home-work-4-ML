{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPpmCkv2hY8J",
        "outputId": "12e32f38-c891-4613-ae29-5abe698f095d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: John enjoys playing football while Mary loves reading books in the library.\n",
            "\n",
            "1. Tokens (Raw): ['John', 'enjoys', 'playing', 'football', 'while', 'Mary', 'loves', 'reading', 'books', 'in', 'the', 'library', '.']\n",
            "2. & 4. Tokens filtered by stop-words/punctuation and POS (Nouns/Verbs):\n",
            "   (Intermediate step not strictly printed, but filtered results are shown below)\n",
            "\n",
            "--- Final Result ---\n",
            "Final Tokens (Lemmatized Nouns and Verbs Only): ['enjoy', 'play', 'football', 'read', 'book', 'library']\n",
            "Input Text: Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\n",
            "\n",
            "--- Named Entity Recognition (NER) Results ---\n",
            "Entity: Chris, Label: PERSON (e.g., People, including fictional)\n",
            "Entity: Alex, Label: PERSON (e.g., People, including fictional)\n",
            "Entity: Apple, Label: ORG (e.g., Companies, agencies, institutions, etc.)\n",
            "Entity: California, Label: GPE (e.g., Countries, cities, states)\n",
            "Entity: iPhone, Label: ORG (e.g., Companies, agencies, institutions, etc.)\n",
            "\n",
            "--- Pronoun Ambiguity Check ---\n",
            "Warning: Possible pronoun ambiguity detected!\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled9.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1NFhRV8TqKwY443qwR_FT8N4iMoHeb5rY\n",
        "\"\"\"\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as english_stopwords\n",
        "\n",
        "# --- NOTE ---\n",
        "# This implementation uses spaCy, which is more robust in environments\n",
        "# where NLTK data downloads fail. It requires the 'en_core_web_sm' model.\n",
        "# If you run into an OSError, you might need to run:\n",
        "# python -m spacy download en_core_web_sm\n",
        "# ------------\n",
        "\n",
        "# Load the small English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"ERROR: spaCy model 'en_core_web_sm' not found.\")\n",
        "    print(\"Please install it by running: python -m spacy download en_core_web_sm\")\n",
        "    exit()\n",
        "\n",
        "# Q1. Input Text\n",
        "input_text = \"John enjoys playing football while Mary loves reading books in the library.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(input_text)\n",
        "\n",
        "# Define the required POS categories (Verbs and Nouns)\n",
        "REQUIRED_POS = {\"VERB\", \"NOUN\"}\n",
        "\n",
        "# 1. Segment into tokens (spaCy handles this automatically during doc processing)\n",
        "# 2. Remove stopwords\n",
        "# 3. Apply lemmatization (spaCy's token.lemma_ attribute)\n",
        "# 4. Keep only verbs and nouns (using token.pos_ attribute)\n",
        "\n",
        "final_tokens = []\n",
        "all_tokens = [token.text for token in doc]\n",
        "\n",
        "# Process tokens sequentially\n",
        "for token in doc:\n",
        "    # 1. Filter out tokens that are punctuation or pure whitespace\n",
        "    if token.is_punct or token.is_space:\n",
        "        continue\n",
        "\n",
        "    # 2. Remove stopwords (using spaCy's defined stop words list)\n",
        "    if token.text.lower() in english_stopwords:\n",
        "        continue\n",
        "\n",
        "    # 4. Keep only verbs and nouns (token.pos_ is the Universal POS tag)\n",
        "    if token.pos_ in REQUIRED_POS:\n",
        "        # 3. Apply lemmatization (token.lemma_ gives the base form)\n",
        "        # We ensure the lemma is converted to lowercase for consistency\n",
        "        lemma = token.lemma_.lower()\n",
        "        final_tokens.append(lemma)\n",
        "\n",
        "\n",
        "print(f\"Input Text: {input_text}\\n\")\n",
        "print(f\"1. Tokens (Raw): {all_tokens}\")\n",
        "print(f\"2. & 4. Tokens filtered by stop-words/punctuation and POS (Nouns/Verbs):\")\n",
        "print(f\"   (Intermediate step not strictly printed, but filtered results are shown below)\")\n",
        "\n",
        "print(\"\\n--- Final Result ---\")\n",
        "# Note: John and Mary are proper nouns, which spaCy correctly lemmatizes to themselves.\n",
        "# 'playing' becomes 'play' (VERB), 'reading' becomes 'read' (VERB).\n",
        "print(f\"Final Tokens (Lemmatized Nouns and Verbs Only): {final_tokens}\")\n",
        "\n",
        "# Expected Output: ['John', 'enjoy', 'play', 'football', 'Mary', 'love', 'read', 'book', 'library']\n",
        "\n",
        "import spacy\n",
        "\n",
        "# --- NOTE ---\n",
        "# You need to install spaCy and download a model if you haven't already:\n",
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "# ------------\n",
        "\n",
        "# Load a pre-trained spaCy model for English\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Error: The spaCy model 'en_core_web_sm' is not found.\")\n",
        "    print(\"Please run: 'python -m spacy download en_core_web_sm'\")\n",
        "    exit()\n",
        "\n",
        "# Q2. Input Text\n",
        "input_text = \"Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch.\"\n",
        "\n",
        "# Process the text with the spaCy model\n",
        "doc = nlp(input_text)\n",
        "\n",
        "# 1. Perform Named Entity Recognition (NER)\n",
        "print(f\"Input Text: {input_text}\\n\")\n",
        "print(\"--- Named Entity Recognition (NER) Results ---\")\n",
        "\n",
        "entities_found = False\n",
        "for ent in doc.ents:\n",
        "    # Use spacy.explain(ent.label_) to provide helpful context for the entity type\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_} (e.g., {spacy.explain(ent.label_)})\")\n",
        "    entities_found = True\n",
        "\n",
        "if not entities_found:\n",
        "    print(\"No named entities were detected.\")\n",
        "\n",
        "\n",
        "# 2. Disambiguation Prompt Check\n",
        "# The required pronouns to check for ambiguity\n",
        "pronouns_to_check = {\"he\", \"she\", \"they\"}\n",
        "ambiguity_detected = False\n",
        "\n",
        "# Iterate through tokens in the document\n",
        "for token in doc:\n",
        "    # Check if the lowercase token text is in our set of pronouns\n",
        "    if token.text.lower() in pronouns_to_check:\n",
        "        ambiguity_detected = True\n",
        "        break\n",
        "\n",
        "print(\"\\n--- Pronoun Ambiguity Check ---\")\n",
        "if ambiguity_detected:\n",
        "    # Print the required warning message\n",
        "    print('Warning: Possible pronoun ambiguity detected!')\n",
        "else:\n",
        "    print('No target pronouns (\"he\", \"she\", \"they\") detected.')"
      ]
    }
  ]
}